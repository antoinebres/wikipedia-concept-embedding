{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from itertools import chain\n",
    "from collections import Counter, OrderedDict\n",
    "import numpy as np\n",
    "import random\n",
    "from keras.layers import Input, Embedding, Dot, Reshape, Dense\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = '../data/found_items_filtered.ndjson'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4661 books.\n"
     ]
    }
   ],
   "source": [
    "books = []\n",
    "\n",
    "with open(raw_data, 'r') as fin:\n",
    "    # Append each line to the books\n",
    "    books = [json.loads(l) for l in fin]\n",
    "\n",
    "# Remove non-book articles\n",
    "books_with_wikipedia = [book for book in books if 'Wikipedia:' in book[0]]\n",
    "books = [book for book in books if 'Wikipedia:' not in book[0]]\n",
    "\n",
    "print(f'Found {len(books)} books.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('WinAPE',\n",
       " {'nom': 'WinAPE',\n",
       "  'développeur': 'Richard Wilson',\n",
       "  'environnement': 'MS Windows',\n",
       "  'langues': 'anglais',\n",
       "  'type': 'Émulateur',\n",
       "  'licence': 'gratuiciel',\n",
       "  'site web': 'www.winape.net'},\n",
       " ['Microsoft Windows',\n",
       "  'Émulateur',\n",
       "  'gratuiciel',\n",
       "  'Microsoft Windows',\n",
       "  'Amstrad CPC'],\n",
       " ['http://www.winape.net',\n",
       "  'http://www.clubic.com/telecharger-fiche14853-winape.html',\n",
       "  'http://cpcrulez.fr/emulateurs_download-WIN-WINAPE.htm',\n",
       "  'http://cpcrulez.fr/emulateurs_download-WIN-WINAPE.htm',\n",
       "  'http://www.dosgamers.com/amstrad-cpc/winape-amstrad-emulator'],\n",
       " '2019-01-06T14:56:41Z',\n",
       " 4329)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 21\n",
    "books[n][0], books[n][1], books[n][2][:5], books[n][3][:5], books[n][4], books[n][5]\n",
    "# 0 title\n",
    "# 1 information from the Infobox template\n",
    "# 2 internal wikipedia links\n",
    "# 3 external links\n",
    "# 4 date of last edit\n",
    "# 5 number of characters in the article (a rough estimate of the length of the article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_index = {book[0]: idx for idx, book in enumerate(books)}\n",
    "index_book = {idx: book for book, idx in book_index.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring internal links (wikilinks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 37751 unique wikilinks.\n"
     ]
    }
   ],
   "source": [
    "wikilinks = list(chain(*[book[2] for book in books]))\n",
    "print(f\"There are {len(set(wikilinks))} unique wikilinks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3073 unique wikilinks to other pages.\n"
     ]
    }
   ],
   "source": [
    "wikilinks_other_books = [link for link in wikilinks if link in book_index.keys()]\n",
    "print(f\"There are {len(set(wikilinks_other_books))} unique wikilinks to other pages.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Linked-to Articles\n",
    "Let's take a look at which pages are most linked to by books on Wikipedia.\n",
    "\n",
    "We'll make a utility function that takes in a list and returns a sorted ordered dictionary of the counts of the items in the list. The collections module has a number of useful functions for dealing with groups of objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_items(l):\n",
    "    \"\"\"Return ordered dictionary of counts of objects in `l`\"\"\"\n",
    "    \n",
    "    # Create a counter object\n",
    "    counts = Counter(l)\n",
    "    \n",
    "    # Sort by highest count first and place in ordered dictionary\n",
    "    counts = sorted(counts.items(), key = lambda x: x[1], reverse = True)\n",
    "    counts = OrderedDict(counts)\n",
    "    \n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('logiciel libre', 1450),\n",
       " ('microsoft windows', 1395),\n",
       " ('linux', 1289),\n",
       " ('catégorie:logiciel libre sous licence gpl', 1036),\n",
       " ('licence publique générale gnu', 949),\n",
       " ('catégorie:logiciel pour windows', 880),\n",
       " ('logiciel propriétaire', 691),\n",
       " ('mac os x', 657),\n",
       " ('open source', 637),\n",
       " ('unix', 617)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find set of wikilinks for each book and convert to a flattened list\n",
    "unique_wikilinks = list(chain(*[list(set(book[2])) for book in books]))\n",
    "wikilinks = [link.lower() for link in unique_wikilinks]\n",
    "\n",
    "wikilink_counts = count_items(wikilinks)\n",
    "list(wikilink_counts.items())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionnal : remove some data (the too common and the too rare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34018\n"
     ]
    }
   ],
   "source": [
    "# to_remove = []\n",
    "# for t in to_remove:\n",
    "#     wikilinks.remove(t)\n",
    "#     _ = wikilink_counts.pop(t)\n",
    "\n",
    "# Limit to greater than 3 links\n",
    "# links = [t[0] for t in wikilink_counts.items() if t[1] >= 4]\n",
    "links = [t[0] for t in wikilink_counts.items()]\n",
    "print(len(links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Microsoft Windows', 1392),\n",
       " ('Linux', 1280),\n",
       " ('C++', 468),\n",
       " ('C (langage)', 436),\n",
       " ('Java (langage)', 408),\n",
       " ('Unix', 402),\n",
       " ('Android', 390),\n",
       " ('Python (langage)', 386),\n",
       " ('GNU', 377),\n",
       " ('JavaScript', 296)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find set of page wikilinks for each page (both a page and a link)\n",
    "unique_wikilinks_books = list(chain(*[list(set(link for link in book[2] if link in book_index.keys())) for book in books]))\n",
    "\n",
    "# Count the number of pages linked to by other pages\n",
    "wikilink_book_counts = count_items(unique_wikilinks_books)\n",
    "list(wikilink_book_counts.items())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Potential Additional Cleaning Step\n",
    "If you want to try more data cleaning, one option would be to clean the link entities. For example, both the new york times and new york times are in the links. These could clearly be combined into a single entry because they link to the same exact page. This might require manual inspection of the links, and I decided not to do this because of the time involved!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wikilinks to Index\n",
    "As with the books, we need to map the Wikilinks to integers. We'll also create the reverse mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_index = {link: idx for idx, link in enumerate(links)}\n",
    "index_link = {idx: link for link, idx in link_index.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Machine Learning Task\n",
    "Now that we have clean data, we'll move on to the second step: developing a supervised machine learning task to train an embedding neural network. As a reminder, we'll state the problem as: given a book title and a link, identify if the link is in the book's article.\n",
    "\n",
    "### Build a Training Set\n",
    "In order for any machine learning model to learn, it needs a training set. We are going to treat this as a supervised learning problem: given a pair (book, link), we want the neural network to learn to predict whether this is a legitimate pair - present in the data - or not.\n",
    "\n",
    "To create a training set, for each book, we'll iterate through the wikilinks on the book page and record the book title and each link as a tuple. The final pairs list will consist of tuples of every (book, link) pairing on all of Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(165570, 34018, 4661)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs = []\n",
    "\n",
    "# Iterate through each book\n",
    "for book in books:\n",
    "    # Iterate through the links in the book\n",
    "    pairs.extend((book_index[book[0]], link_index[link.lower()]) for link in book[2] if link.lower() in links)\n",
    "    \n",
    "len(pairs), len(links), len(books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_set = set(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(100)\n",
    "\n",
    "def generate_batch(pairs, n_positive = 50, negative_ratio = 1.0, classification = False):\n",
    "    \"\"\"Generate batches of samples for training\"\"\"\n",
    "    batch_size = n_positive * (1 + negative_ratio)\n",
    "    batch = np.zeros((batch_size, 3))\n",
    "    \n",
    "    # Adjust label based on task\n",
    "    if classification:\n",
    "        neg_label = 0\n",
    "    else:\n",
    "        neg_label = -1\n",
    "    \n",
    "    # This creates a generator\n",
    "    while True:\n",
    "        # randomly choose positive examples\n",
    "        for idx, (book_id, link_id) in enumerate(random.sample(pairs, n_positive)):\n",
    "            batch[idx, :] = (book_id, link_id, 1)\n",
    "\n",
    "        # Increment idx by 1\n",
    "        idx += 1\n",
    "        \n",
    "        # Add negative examples until reach batch size\n",
    "        while idx < batch_size:\n",
    "            \n",
    "            # random selection\n",
    "            random_book = random.randrange(len(books))\n",
    "            random_link = random.randrange(len(links))\n",
    "            \n",
    "            # Check to make sure this is not a positive example\n",
    "            if (random_book, random_link) not in pairs_set:\n",
    "                \n",
    "                # Add to batch and increment index\n",
    "                batch[idx, :] = (random_book, random_link, neg_label)\n",
    "                idx += 1\n",
    "                \n",
    "        # Make sure to shuffle order\n",
    "        np.random.shuffle(batch)\n",
    "        yield {'book': batch[:, 0], 'link': batch[:, 1]}, batch[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page: KHTML                          Link: qcow2                                    Label: -1.0\n",
      "Page: OpenOffice.org                 Link: opentoonz                                Label: -1.0\n",
      "Page: Qt                             Link: python (langage)                         Label: 1.0\n",
      "Page: PowerDVD                       Link: ukui                                     Label: -1.0\n",
      "Page: VirtueMart                     Link: récepteur nucléaire                      Label: -1.0\n",
      "Page: Wpa supplicant                 Link: catégorie:logiciel pour linux            Label: 1.0\n"
     ]
    }
   ],
   "source": [
    "x, y = next(generate_batch(pairs, n_positive = 2, negative_ratio = 2))\n",
    "\n",
    "# Show a few example training pairs\n",
    "for label, b_idx, l_idx in zip(y, x['book'], x['link']):\n",
    "    print(f'Page: {index_book[b_idx]:30} Link: {index_link[l_idx]:40} Label: {label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34018"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(link_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def book_embedding_model(embedding_size = 50, classification = False):\n",
    "    \"\"\"Model to embed books and wikilinks using the functional API.\n",
    "       Trained to discern if a link is present in a article\"\"\"\n",
    "    \n",
    "    # Both inputs are 1-dimensional\n",
    "    book = Input(name = 'book', shape = [1])\n",
    "    link = Input(name = 'link', shape = [1])\n",
    "    \n",
    "    # Embedding the book (shape will be (None, 1, 50))\n",
    "    book_embedding = Embedding(name = 'book_embedding',\n",
    "                               input_dim = len(book_index),\n",
    "                               output_dim = embedding_size)(book)\n",
    "    \n",
    "    # Embedding the link (shape will be (None, 1, 50))\n",
    "    link_embedding = Embedding(name = 'link_embedding',\n",
    "                               input_dim = len(link_index),\n",
    "                               output_dim = embedding_size)(link)\n",
    "    \n",
    "    # Merge the layers with a dot product along the second axis (shape will be (None, 1, 1))\n",
    "    merged = Dot(name = 'dot_product', normalize = True, axes = 2)([book_embedding, link_embedding])\n",
    "    \n",
    "    # Reshape to be a single number (shape will be (None, 1))\n",
    "    merged = Reshape(target_shape = [1])(merged)\n",
    "    \n",
    "    # If classifcation, add extra layer and loss function is binary cross entropy\n",
    "    if classification:\n",
    "        merged = Dense(1, activation = 'sigmoid')(merged)\n",
    "        model = Model(inputs = [book, link], outputs = merged)\n",
    "        model.compile(optimizer = 'Adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    # Otherwise loss function is mean squared error\n",
    "    else:\n",
    "        model = Model(inputs = [book, link], outputs = merged)\n",
    "        model.compile(optimizer = 'Adam', loss = 'mse')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = '../models/'\n",
    "\n",
    "save_emb_dir = '../saved_embeddings/items/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_positive = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/antoine.bres/miniconda3/envs/ds/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/11\n",
      " - 4s - loss: 0.9822\n",
      "Epoch 2/11\n",
      " - 4s - loss: 0.9117\n",
      "Epoch 3/11\n",
      " - 4s - loss: 0.7621\n",
      "Epoch 4/11\n",
      " - 4s - loss: 0.5807\n",
      "Epoch 5/11\n",
      " - 4s - loss: 0.5053\n",
      "Epoch 6/11\n",
      " - 4s - loss: 0.4820\n",
      "Epoch 7/11\n",
      " - 4s - loss: 0.4374\n",
      "Epoch 8/11\n",
      " - 4s - loss: 0.4248\n",
      "Epoch 9/11\n",
      " - 4s - loss: 0.4205\n",
      "Epoch 10/11\n",
      " - 4s - loss: 0.4360\n",
      "Epoch 11/11\n",
      " - 4s - loss: 0.4086\n"
     ]
    }
   ],
   "source": [
    "# Instantiate model and show parameters\n",
    "model = book_embedding_model(50)\n",
    "gen = generate_batch(pairs, n_positive, negative_ratio = 2)\n",
    "h = model.fit_generator(gen, epochs = 11, steps_per_epoch = len(pairs) // n_positive, verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/antoine.bres/miniconda3/envs/ds/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      " - 3s - loss: 0.6730 - accuracy: 0.6025\n",
      "Epoch 2/20\n",
      " - 4s - loss: 0.6165 - accuracy: 0.7668\n",
      "Epoch 3/20\n",
      " - 4s - loss: 0.5263 - accuracy: 0.8479\n",
      "Epoch 4/20\n",
      " - 4s - loss: 0.4450 - accuracy: 0.8485\n",
      "Epoch 5/20\n",
      " - 3s - loss: 0.3808 - accuracy: 0.8723\n",
      "Epoch 6/20\n",
      " - 3s - loss: 0.3687 - accuracy: 0.8733\n",
      "Epoch 7/20\n",
      " - 4s - loss: 0.3459 - accuracy: 0.8862\n",
      "Epoch 8/20\n",
      " - 5s - loss: 0.3402 - accuracy: 0.8930\n",
      "Epoch 9/20\n",
      " - 4s - loss: 0.3558 - accuracy: 0.8822\n",
      "Epoch 10/20\n",
      " - 4s - loss: 0.3213 - accuracy: 0.9063\n",
      "Epoch 11/20\n",
      " - 4s - loss: 0.3256 - accuracy: 0.9013\n",
      "Epoch 12/20\n",
      " - 4s - loss: 0.3319 - accuracy: 0.8966\n",
      "Epoch 13/20\n",
      " - 3s - loss: 0.2683 - accuracy: 0.9346\n",
      "Epoch 14/20\n",
      " - 4s - loss: 0.3330 - accuracy: 0.8975\n",
      "Epoch 15/20\n",
      " - 3s - loss: 0.2954 - accuracy: 0.9179\n",
      "Epoch 16/20\n",
      " - 3s - loss: 0.2462 - accuracy: 0.9431\n",
      "Epoch 17/20\n",
      " - 4s - loss: 0.2796 - accuracy: 0.9248\n",
      "Epoch 18/20\n",
      " - 3s - loss: 0.2830 - accuracy: 0.9220\n",
      "Epoch 19/20\n",
      " - 3s - loss: 0.2479 - accuracy: 0.9401\n",
      "Epoch 20/20\n",
      " - 3s - loss: 0.2354 - accuracy: 0.9474\n"
     ]
    }
   ],
   "source": [
    "model_class = book_embedding_model(50, classification = True)\n",
    "gen = generate_batch(pairs, n_positive, negative_ratio=2, classification = True)\n",
    "h = model_class.fit_generator(gen, epochs = 20, steps_per_epoch= len(pairs) // n_positive, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_weights(name, model):\n",
    "    \"\"\"Extract weights from a neural network model\"\"\"\n",
    "    \n",
    "    # Extract weights\n",
    "    weight_layer = model.get_layer(name)\n",
    "    weights = weight_layer.get_weights()[0]\n",
    "    \n",
    "    # Normalize\n",
    "    weights = weights / np.linalg.norm(weights, axis = 1).reshape((-1, 1))\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_all(model, model_name, save_emb_dir, pages, links):\n",
    "    # save model\n",
    "    model.save(models_dir + model_name)\n",
    "    # save page embedding\n",
    "    np.savetxt(save_emb_dir + 'page_embedding.tsv', extract_weights('book_embedding', model), delimiter='\\t')\n",
    "    # save page names\n",
    "    save_as_tsv(save_emb_dir + 'page_names.tsv', pages)\n",
    "    # save links embedding\n",
    "    np.savetxt(save_emb_dir + 'link_embedding.tsv', extract_weights('link_embedding', model), delimiter='\\t')\n",
    "    # save links names\n",
    "    save_as_tsv(save_emb_dir + 'link_names.tsv', links)\n",
    "\n",
    "def save_as_tsv(path, data):\n",
    "    with open(path, 'w' , encoding = 'utf-8') as f:\n",
    "        for l in data:\n",
    "            f.write(str(l) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = book_index.keys()\n",
    "links = link_index.keys()\n",
    "\n",
    "save_all(model, 'first_attempt_items.h5', save_emb_dir, pages, links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = book_index.keys()\n",
    "links = link_index.keys()\n",
    "\n",
    "save_all(model_class, 'first_attempt_class_items.h5', save_emb_dir, pages, links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('ds': conda)",
   "language": "python",
   "name": "python361064bitdsconda12cc25f9d69d44a399eabbfeb231183e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
